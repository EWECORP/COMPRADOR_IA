{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparaci칩n de los Datos\n",
    "\n",
    "Consulta Ajustada\n",
    "\n",
    "Dado que deseas capturar la estacionalidad, es necesario incluir datos del mismo mes del a침o anterior. Aseg칰rate de incluir el rango temporal necesario en la consulta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FECHA_CORTE: Debe ser 30 d칤as a la fecha actual --> La vamos a usar como l칤mite del entrenamiento del modelo\n",
    "### FECHA_FUTURA: Vamos a usar los 30 d칤as reales para comparar contra el real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FECHA_CORTE = '20241130'\n",
    "FECHA_FUTURA = '20241230'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Para Funcionas asegurarse de conectarse a la VPN\n",
    "\n",
    "Pass: Z33treex.2024\t\n",
    "\n",
    "DIARCOEST -->  SQLSERVER:192.168.0.250   eettlin/connexa.2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRIVER=ODBC Driver 17 for SQL Server;SERVER =192.168.0.250;BASE=DiarcoEst;USER=eettlin;PWD=connexa.2024\n"
     ]
    }
   ],
   "source": [
    "# OBTENER PAR츼METROS de la CONEXI칍N\n",
    "\n",
    "import pyodbc\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "secrets = dotenv_values(\".env\")\n",
    "local_secrets = dotenv_values(\".env.dev\")\n",
    "\n",
    "DRIVER = secrets[\"DRIVER\"]\n",
    "SERVIDOR = secrets[\"SERVIDOR\"]\n",
    "PUERTO = secrets[\"PUERTO\"]\n",
    "BASE = secrets[\"BASE\"]\n",
    "USUARIO = secrets[\"USUARIO\"]\n",
    "CONTRASENA = secrets[\"CONTRASENA\"]\n",
    "\n",
    "\n",
    "constr = f'DRIVER={DRIVER};SERVER ={SERVIDOR};BASE={BASE};USER={USUARIO};PWD={CONTRASENA}'\n",
    "\n",
    "print (constr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eduar\\AppData\\Local\\Temp\\ipykernel_34384\\1206587002.py:37: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# Configuraci칩n de conexi칩n\n",
    "constr = f'DRIVER={DRIVER};SERVER ={SERVIDOR};PORT={PUERTO};BASE={BASE};USER={USUARIO};PWD={CONTRASENA}'\n",
    "\n",
    "conn_str = (\n",
    "    \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    \"SERVER=192.168.0.250;\"\n",
    "     \"PORT=1433;\"\n",
    "    \"DATABASE=DiarcoEst;\"\n",
    "    \"UID=eettlin;\"\n",
    "    \"PWD=connexa.2024;\"\n",
    ")\n",
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "query = \"\"\"\n",
    "-- Consulta con datos hist칩ricos y actuales\n",
    "SELECT V.[F_VENTA],\n",
    "       V.[C_ARTICULO],\n",
    "       V.[C_SUCU_EMPR],\n",
    "       V.[Q_UNIDADES_VENDIDAS],\n",
    "       V.[I_PRECIO_VENTA],\n",
    "       V.[I_PRECIO_COSTO],\n",
    "       V.[I_VENDIDO],\n",
    "       V.[I_PRECIO_COSTO_PP],\n",
    "       A.[C_FAMILIA],\n",
    "       A.[C_RUBRO],\n",
    "       A.[C_SUBRUBRO_1],\n",
    "       A.[C_SUBRUBRO_2],\n",
    "       A.[N_ARTICULO]\n",
    "FROM [DiarcoEst].[dbo].[T702_EST_VTAS_POR_ARTICULO] V\n",
    "LEFT JOIN [DiarcoEst].[dbo].[T050_ARTICULOS] A \n",
    "    ON V.C_ARTICULO = A.C_ARTICULO\n",
    "WHERE V.C_ARTICULO BETWEEN  50 AND 200  AND V.F_VENTA >= DATEADD(YEAR, -1, '20241130')\n",
    "\"\"\"\n",
    "data = pd.read_sql(query, conn)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ingenier칤a de Caracter칤sticas\n",
    "Una vez cargados los datos, realiza los siguientes pasos para crear caracter칤sticas relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2.1 Variables Temporales\n",
    "# Extrae informaci칩n adicional de la columna F_VENTA:\n",
    "\n",
    "data['F_VENTA'] = pd.to_datetime(data['F_VENTA'])\n",
    "data['mes'] = data['F_VENTA'].dt.month\n",
    "data['dia_semana'] = data['F_VENTA'].dt.dayofweek\n",
    "data['es_fin_semana'] = data['dia_semana'].isin([5, 6]).astype(int)\n",
    "data['a침o'] = data['F_VENTA'].dt.year\n",
    "\n",
    "\n",
    "# 2.2 Precio Relativo\n",
    "# Calcula el precio relativo respecto al promedio hist칩rico del art칤culo:\n",
    "\n",
    "data = data[data['I_PRECIO_COSTO_PP'] != 0]    # Filtrar Datos sin Costo para que no de Overflow\n",
    "data['precio_relativo'] = data['I_PRECIO_VENTA'] / data['I_PRECIO_COSTO_PP']\n",
    "\n",
    "# 2.3 Estacionalidad\n",
    "# Agrupa las ventas por mes para detectar tendencias estacionales:\n",
    "\n",
    "estacionalidad = data.groupby(['mes', 'C_ARTICULO'])['Q_UNIDADES_VENDIDAS'].mean().reset_index()\n",
    "data = pd.merge(data, estacionalidad, on=['mes', 'C_ARTICULO'], suffixes=('', '_estacional'))\n",
    "\n",
    "#2.4 Variables Categ칩ricas\n",
    "#Convierte las categor칤as en variables dummy:\n",
    "\n",
    "data = pd.get_dummies(data, columns=['C_FAMILIA', 'C_RUBRO', 'C_SUBRUBRO_1', 'C_SUBRUBRO_2'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelado\n",
    "Probaremos varios algoritmos para estimar la demanda.\n",
    "\n",
    "### 3.1 Dividir Datos\n",
    "\n",
    "Define las variables independientes ( 洧녦 ) y dependientes (洧녽):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['precio_relativo', 'es_fin_semana', 'dia_semana', 'Q_UNIDADES_VENDIDAS_estacional']]\n",
    "y = data['Q_UNIDADES_VENDIDAS']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVISAR CALIDAD de los DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores NaN en X_train: False\n",
      "Valores Inf en X_train: False\n",
      "Valores m치ximos en X_train: precio_relativo                   7380.299000\n",
      "es_fin_semana                        1.000000\n",
      "dia_semana                           6.000000\n",
      "Q_UNIDADES_VENDIDAS_estacional     213.384615\n",
      "dtype: float64\n",
      "Valores m칤nimos en X_train: precio_relativo                   0.245946\n",
      "es_fin_semana                     0.000000\n",
      "dia_semana                        0.000000\n",
      "Q_UNIDADES_VENDIDAS_estacional    0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Verificar NaN\n",
    "print(\"Valores NaN en X_train:\", np.any(np.isnan(X_train)))\n",
    "\n",
    "# Verificar Inf\n",
    "print(\"Valores Inf en X_train:\", np.any(np.isinf(X_train)))\n",
    "\n",
    "# Verificar valores extremos\n",
    "print(\"Valores m치ximos en X_train:\", np.max(X_train, axis=0))\n",
    "print(\"Valores m칤nimos en X_train:\", np.min(X_train, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustar los Datos si hay Errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Revisar y manejar valores problem치ticos\n",
    "X_train = np.nan_to_num(X_train, nan=np.nanmean(X_train, axis=0))  # Rellenar NaN\n",
    "X_train = np.where(np.isinf(X_train), np.nanmax(X_train), X_train)  # Rellenar Inf\n",
    "\n",
    "# Normalizar valores grandes\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultimo Control de Todo Bien "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar que no haya valores NaN ni Inf en X_train\n",
    "assert not np.any(np.isnan(X_train)), \"A칰n hay valores NaN en X_train\"\n",
    "assert not np.any(np.isinf(X_train)), \"A칰n hay valores Inf en X_train\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Modelos a Probar\n",
    "\n",
    "#### Regresi칩n Lineal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\PY\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but LinearRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "y_pred_lr = model_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\PY\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but RandomForestRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "y_pred_rf = model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "model_xgb = XGBRegressor(random_state=42)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluaci칩n de los Modelos\n",
    "Compara los resultados usando m칠tricas como RMSE o 洧녠 2R 2\n",
    " :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regresi칩n Lineal - RMSE: 1650.50, R^2: -117.72\n",
      "Random Forest - RMSE: 152.29, R^2: -0.01\n",
      "XGBoost - RMSE: 542.02, R^2: -11.80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluar_modelo(y_test, y_pred, nombre):\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{nombre} - RMSE: {rmse:.2f}, R^2: {r2:.2f}\")\n",
    "\n",
    "evaluar_modelo(y_test, y_pred_lr, \"Regresi칩n Lineal\")\n",
    "evaluar_modelo(y_test, y_pred_rf, \"Random Forest\")\n",
    "evaluar_modelo(y_test, y_pred_xgb, \"XGBoost\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predicci칩n de Demanda Futura\n",
    "\n",
    "Una vez que el mejor modelo est칠 identificado, puedes usarlo para predecir la demanda futura generando un conjunto de datos con las fechas pr칩ximas y las caracter칤sticas calculadas de manera similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      F_VENTA  C_ARTICULO  pred_demanda  I_PRECIO_VENTA\n",
      "0  2024-12-01        52.0     39.736833     1242.433052\n",
      "1  2024-12-02        52.0     30.066667     1242.433052\n",
      "2  2024-12-03        52.0     10.753667     1242.433052\n",
      "3  2024-12-04        52.0     14.481667     1242.433052\n",
      "4  2024-12-05        52.0     14.481667     1242.433052\n",
      "5  2024-12-06        52.0     14.481667     1242.433052\n",
      "6  2024-12-07        52.0     39.736833     1242.433052\n",
      "7  2024-12-08        52.0     39.736833     1242.433052\n",
      "8  2024-12-09        52.0     30.066667     1242.433052\n",
      "9  2024-12-10        52.0     10.753667     1242.433052\n",
      "10 2024-12-11        52.0     14.481667     1242.433052\n",
      "11 2024-12-12        52.0     14.481667     1242.433052\n",
      "12 2024-12-13        52.0     14.481667     1242.433052\n",
      "13 2024-12-14        52.0     39.736833     1242.433052\n",
      "14 2024-12-15        52.0     39.736833     1242.433052\n",
      "15 2024-12-16        52.0     30.066667     1242.433052\n",
      "16 2024-12-17        52.0     10.753667     1242.433052\n",
      "17 2024-12-18        52.0     14.481667     1242.433052\n",
      "18 2024-12-19        52.0     14.481667     1242.433052\n",
      "19 2024-12-20        52.0     14.481667     1242.433052\n",
      "20 2024-12-21        52.0     39.736833     1242.433052\n",
      "21 2024-12-22        52.0     39.736833     1242.433052\n",
      "22 2024-12-23        52.0     30.066667     1242.433052\n",
      "23 2024-12-24        52.0     10.753667     1242.433052\n",
      "24 2024-12-25        52.0     14.481667     1242.433052\n",
      "25 2024-12-26        52.0     14.481667     1242.433052\n",
      "26 2024-12-27        52.0     14.481667     1242.433052\n",
      "27 2024-12-28        52.0     39.736833     1242.433052\n",
      "28 2024-12-29        52.0     39.736833     1242.433052\n",
      "29 2024-12-30        52.0     30.066667     1242.433052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\PY\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but RandomForestRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame de fechas futuras\n",
    "future_data = pd.DataFrame({\n",
    "    'F_VENTA': pd.date_range(start='2024-12-01', periods=30, freq='D'),\n",
    "    'mes': pd.date_range(start='2024-12-01', periods=30, freq='D').month,\n",
    "    'dia_semana': pd.date_range(start='2024-12-01', periods=30, freq='D').dayofweek,\n",
    "})\n",
    "future_data['es_fin_semana'] = future_data['dia_semana'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Asociar datos futuros con los art칤culos (puedes duplicar filas para cada art칤culo)\n",
    "future_data = future_data.assign(C_ARTICULO=data['C_ARTICULO'].unique()[0])  # Cambiar para manejar m칰ltiples art칤culos\n",
    "\n",
    "# Calcular precio relativo (basado en datos hist칩ricos)\n",
    "future_data = future_data.merge(\n",
    "    data.groupby('C_ARTICULO')['I_PRECIO_COSTO_PP'].mean().reset_index().rename(columns={'I_PRECIO_COSTO_PP': 'precio_regular'}),\n",
    "    on='C_ARTICULO',\n",
    "    how='left'\n",
    ")\n",
    "future_data['I_PRECIO_VENTA'] = data.groupby('C_ARTICULO')['I_PRECIO_VENTA'].transform('mean')\n",
    "future_data['precio_relativo'] = future_data['I_PRECIO_VENTA'] / future_data['precio_regular']\n",
    "\n",
    "# Asociar estacionalidad\n",
    "estacionalidad = data.groupby(['mes', 'C_ARTICULO'])['Q_UNIDADES_VENDIDAS'].mean().reset_index()\n",
    "estacionalidad.rename(columns={'Q_UNIDADES_VENDIDAS': 'Q_UNIDADES_VENDIDAS_estacional'}, inplace=True)\n",
    "future_data = pd.merge(future_data, estacionalidad, on=['mes', 'C_ARTICULO'], how='left')\n",
    "\n",
    "# Realizar predicciones\n",
    "future_data['pred_demanda'] = model_rf.predict(future_data[['precio_relativo', 'es_fin_semana', 'dia_semana', 'Q_UNIDADES_VENDIDAS_estacional']])\n",
    "\n",
    "# Mostrar resultados\n",
    "print(future_data[['F_VENTA', 'C_ARTICULO', 'pred_demanda','I_PRECIO_VENTA']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMULARI칍N COMPLETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Passing 'suffixes' which cause duplicate columns {'precio_regular_x'} is not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m precios\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI_PRECIO_COSTO_PP\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecio_regular\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI_PRECIO_VENTA\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecio_promedio\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Asociar precios al conjunto futuro\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m future_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecios\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC_ARTICULO\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m future_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecio_relativo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m future_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecio_promedio\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m future_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecio_regular\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 5. Predicci칩n de la demanda futura\u001b[39;00m\n",
      "File \u001b[1;32me:\\PY\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m         left_df,\n\u001b[0;32m    172\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\PY\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:888\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[0;32m    886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[1;32m--> 888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[1;32me:\\PY\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:840\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    837\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft[:]\n\u001b[0;32m    838\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright[:]\n\u001b[1;32m--> 840\u001b[0m llabels, rlabels \u001b[38;5;241m=\u001b[39m \u001b[43m_items_overlap_with_suffix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuffixes\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[0;32m    848\u001b[0m     lmgr \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    849\u001b[0m         join_index,\n\u001b[0;32m    850\u001b[0m         left_indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    856\u001b[0m     )\n",
      "File \u001b[1;32me:\\PY\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2757\u001b[0m, in \u001b[0;36m_items_overlap_with_suffix\u001b[1;34m(left, right, suffixes)\u001b[0m\n\u001b[0;32m   2755\u001b[0m     dups\u001b[38;5;241m.\u001b[39mextend(rlabels[(rlabels\u001b[38;5;241m.\u001b[39mduplicated()) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;241m~\u001b[39mright\u001b[38;5;241m.\u001b[39mduplicated())]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m   2756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dups:\n\u001b[1;32m-> 2757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[0;32m   2758\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuffixes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m which cause duplicate columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(dups)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2759\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2760\u001b[0m     )\n\u001b[0;32m   2762\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m llabels, rlabels\n",
      "\u001b[1;31mMergeError\u001b[0m: Passing 'suffixes' which cause duplicate columns {'precio_regular_x'} is not allowed."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcular precio relativo promedio por art칤culo\n",
    "precios = data.groupby('C_ARTICULO')[['I_PRECIO_COSTO_PP', 'I_PRECIO_VENTA']].mean().reset_index()\n",
    "precios.rename(columns={'I_PRECIO_COSTO_PP': 'precio_regular', 'I_PRECIO_VENTA': 'precio_promedio'}, inplace=True)\n",
    "\n",
    "# Asociar precios al conjunto futuro\n",
    "future_data = pd.merge(future_data, precios, on='C_ARTICULO', how='left')\n",
    "future_data['precio_relativo'] = future_data['precio_promedio'] / future_data['precio_regular']\n",
    "\n",
    "# 5. Predicci칩n de la demanda futura\n",
    "X_future = future_data[['precio_relativo', 'es_fin_semana', 'dia_semana', 'mes']]\n",
    "future_data['pred_demanda'] = model_rf.predict(X_future)\n",
    "\n",
    "# 6. Comparar con datos reales\n",
    "query_real = \"\"\"\n",
    "    SELECT F_VENTA, C_ARTICULO, Q_UNIDADES_VENDIDAS\n",
    "    FROM [DiarcoEst].[dbo].[T702_EST_VTAS_POR_ARTICULO]\n",
    "    WHERE C_ARTICULO BETWEEN  50 AND 100  AND \n",
    "        F_VENTA BETWEEN CONVERT(DATE, '20241130') AND DATEADD(DAY, 30, CONVERT(DATE, '20241130'))\n",
    "\"\"\"\n",
    "real_data = pd.read_sql(query_real, conn)\n",
    "comparacion = pd.merge(future_data, real_data, on=['F_VENTA', 'C_ARTICULO'], how='left')\n",
    "comparacion['error'] = comparacion['pred_demanda'] - comparacion['Q_UNIDADES_VENDIDAS']\n",
    "\n",
    "# 7. M칠tricas de evaluaci칩n\n",
    "mae = mean_absolute_error(comparacion['Q_UNIDADES_VENDIDAS'].dropna(), comparacion['pred_demanda'])\n",
    "rmse = np.sqrt(mean_squared_error(comparacion['Q_UNIDADES_VENDIDAS'].dropna(), comparacion['pred_demanda']))\n",
    "print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# 8. Visualizaci칩n de resultados\n",
    "articulo_especifico = data['C_ARTICULO'].unique()[0]\n",
    "datos_articulo = comparacion[comparacion['C_ARTICULO'] == articulo_especifico]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(datos_articulo['F_VENTA'], datos_articulo['pred_demanda'], label='Predicci칩n', marker='o')\n",
    "plt.plot(datos_articulo['F_VENTA'], datos_articulo['Q_UNIDADES_VENDIDAS'], label='Real', marker='x')\n",
    "plt.title(f'Predicci칩n vs Real para Art칤culo {articulo_especifico}')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Demanda')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
