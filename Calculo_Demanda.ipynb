{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para estimar la demanda de productos en una cadena de supermercados considerando los factores mencionados (stock, ofertas y ventas históricas), se puede desarrollar un modelo utilizando Python y herramientas de análisis de datos y aprendizaje automático. A continuación, se describe cómo abordar el problema, junto con un modelo conceptual y el uso de las herramientas necesarias:\n",
    "## 1. Enfoque General\n",
    "    a. Objetivo\n",
    "    Estimar la demanda diaria esperada de cada producto en cada tienda, ajustada por:\n",
    "        •\tDisponibilidad de stock.\n",
    "        •\tImpacto de las ofertas.\n",
    "        •\tTendencias de ventas históricas.\n",
    "    b. Consideraciones\n",
    "        •\tUsar únicamente datos de días con stock suficiente para calcular el promedio y modelar la demanda base.\n",
    "        •\tCapturar el impacto de las ofertas como un multiplicador en la demanda base.\n",
    "        •\tIncorporar estacionalidad y patrones semanales.\n",
    "    c. Herramientas\n",
    "        •\tPandas: Para manipulación y limpieza de datos.\n",
    "        •\tScikit-learn: Para modelos de regresión lineal y Machine Learning.\n",
    "        •\tStatsmodels: Para modelado de series temporales (opcional).\n",
    "        •\tNumPy y Matplotlib/Seaborn: Para cálculos y visualización.\n",
    "\n",
    "## 2. Diseño del Modelo\n",
    "### a. Variables de Entrada\n",
    "1.\tFecha: Día específico.\n",
    "2.\tTienda: Identificador de la tienda.\n",
    "3.\tProducto: Identificador del producto.\n",
    "4.\tVentas Diarias: Cantidad vendida en ese día.\n",
    "5.\tStock Suficiente (Booleano): Indicador de si había suficiente stock.\n",
    "6.\tEn Oferta (Booleano): Indicador de si el producto estaba en oferta.\n",
    "7.\tPrecio Relativo: Precio del producto en comparación con su precio regular.\n",
    "    El precio relativo es un dato numérico continuo y se utiliza en modelos de predicción como una variable independiente. Es importante normalizar o estandarizar esta variable si se emplea en modelos de Machine Learning que son sensibles a la escala.\n",
    "\n",
    "    Precio Actual / Precio Regular\n",
    "\n",
    "        **Formato de Dato** \n",
    "        Tipo de dato en Python: float\n",
    "        Valores típicos:\n",
    "        < 1.0: Indica descuentos.\n",
    "        1.0: Precio igual al precio regular.\n",
    "        > 1.0: Precio más alto que el regular.\n",
    "8.\tCaracterísticas Externas: Festivos, eventos locales, etc.\n",
    "\n",
    "### b. Preparación de Datos\n",
    "\n",
    "1.\tFiltrar días sin suficiente stock para estimar la demanda base.\n",
    "\n",
    "2.\tCalcular el promedio de ventas diarias por producto-tienda para los días con stock suficiente.\n",
    "\n",
    "3.\tCodificar información sobre ofertas y ajustar la demanda observada.\n",
    "\n",
    "## 3. Modelo Basado en Regresión\n",
    "1.\tRegresión Lineal Múltiple Usar variables como precio relativo, indicador de oferta, día de la semana, y festividad para estimar la demanda diaria.\n",
    "Fórmula básica:\n",
    " \n",
    "Demandat=β0+β1(Ofertat)+β2(Precio Relativot)+β3(Dıˊa de la Semanat)+ϵ\\text{Demanda}_t = \\beta_0 + \\beta_1 (\\text{Oferta}_t) + \\beta_2 (\\text{Precio Relativo}_t) + \\beta_3 (\\text{Día de la Semana}_t) + \\epsilonDemandat=β0+β1(Ofertat)+β2(Precio Relativot)+β3(Dıˊa de la Semanat)+ϵ\n",
    "\n",
    "2.\tAlgoritmos de ML Adicionales\n",
    "o\tRandom Forest: Para modelar relaciones no lineales y manejar interacciones complejas entre variables.\n",
    "o\tXGBoost: Para mejorar la precisión en estimaciones más complejas.\n",
    "3.\tValidación del Modelo Dividir los datos en conjuntos de entrenamiento y prueba. Utilizar métricas como el RMSE o MAE para evaluar la precisión.\n",
    "\n",
    "4. Implementación en Python\n",
    "A continuación, un esquema básico del código:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv(\"ventas_historicas.csv\")\n",
    "\n",
    "# Filtrar días con stock suficiente\n",
    "data = data[data['stock_suficiente'] == 1]\n",
    "\n",
    "# Crear variables de tiempo\n",
    "data['day_of_week'] = pd.to_datetime(data['fecha']).dt.dayofweek\n",
    "data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Variables dummies para ofertas y estacionalidad\n",
    "data['oferta'] = data['oferta'].astype(int)\n",
    "data = pd.get_dummies(data, columns=['day_of_week'], drop_first=True)\n",
    "\n",
    "# Definir variables independientes y dependientes\n",
    "X = data[['oferta', 'precio_relativo', 'is_weekend'] + [col for col in data.columns if 'day_of_week_' in col]]\n",
    "y = data['ventas']\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar un modelo de Random Forest\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones y evaluación\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"RMSE del modelo: {rmse}\")\n",
    "\n",
    "# Importancia de las variables\n",
    "importances = model.feature_importances_\n",
    "plt.barh(X.columns, importances)\n",
    "plt.xlabel(\"Importancia\")\n",
    "plt.ylabel(\"Variables\")\n",
    "plt.title(\"Importancia de las Variables en el Modelo\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizar POSTGRE para Acelerar Cálculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Configurar la conexión\n",
    "db_config = {\n",
    "    \"host\": \"tu_host\",\n",
    "    \"port\": \"5432\",  # Puerto por defecto\n",
    "    \"dbname\": \"nombre_base_datos\",\n",
    "    \"user\": \"tu_usuario\",\n",
    "    \"password\": \"tu_contraseña\"\n",
    "}\n",
    "\n",
    "# Crear la conexión usando SQLAlchemy\n",
    "engine = create_engine(\n",
    "    f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\"\n",
    ")\n",
    "\n",
    "# Probar la conexión y cargar datos directamente\n",
    "query = \"\"\"\n",
    "SELECT fecha, tienda, producto, ventas, stock_suficiente, oferta, precio_relativo\n",
    "FROM ventas_historicas\n",
    "WHERE stock_suficiente = true;  -- Filtrar días con stock suficiente\n",
    "\"\"\"\n",
    "\n",
    "# Cargar datos en un DataFrame de pandas\n",
    "data = pd.read_sql(query, engine)\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesar Datos en la Base de Datos \n",
    "Aprovecha la potencia de PostgreSQL para realizar cálculos y filtrados directamente en la base de datos antes de cargar los datos en Python. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql \n",
    "SELECT tienda, producto, AVG(ventas) AS promedio_ventas\n",
    "FROM ventas_historicas\n",
    "WHERE stock_suficiente = true\n",
    "GROUP BY tienda, producto;\n",
    "\n",
    "#sql\n",
    "SELECT tienda, producto, AVG(ventas) AS promedio_ventas\n",
    "FROM ventas_historicas\n",
    "WHERE stock_suficiente = true\n",
    "GROUP BY tienda, producto;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga Parcial de Datos \n",
    "Si los datos son demasiado grandes para cargarlos todos a la vez, puedes dividirlos en \"chunks\" (porciones) utilizando la función chunksize de pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in pd.read_sql(query, engine, chunksize=10000):\n",
    "    # Procesar cada porción por separado\n",
    "    procesar_chunk(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporar Estacionalidad con PROPHET\n",
    "\n",
    "Prophet es especialmente útil para capturar estacionalidades, tendencias y efectos de días festivos. En tu caso, puedes añadir los días festivos de Argentina como una de las entradas del modelo.\n",
    "\n",
    "### Preparación de los Datos\n",
    "Asegúrate de tener los datos en un formato adecuado para Prophet. Necesitas al menos dos columnas:\n",
    "\n",
    "* ds: Fecha (en formato YYYY-MM-DD).\n",
    "* y: Variable objetivo (en este caso, la demanda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar datos desde PostgreSQL o un archivo\n",
    "data = pd.DataFrame({\n",
    "    'ds': ['2024-01-01', '2024-01-02', '2024-01-03'],  # Fechas\n",
    "    'y': [100, 120, 110]  # Ventas\n",
    "})\n",
    "\n",
    "# Convertir 'ds' al formato de fecha\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporar los Días Festivos de Argentina\n",
    "Prophet permite añadir efectos de días festivos a nivel nacional. Puedes usar un paquete como holidays para obtener los feriados locales de Argentina:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "# Obtener los días festivos de Argentina\n",
    "argentina_holidays = holidays.AR(years=[2023, 2024])  # Ajusta los años necesarios\n",
    "\n",
    "# Crear un DataFrame con los días festivos\n",
    "holidays_df = pd.DataFrame({\n",
    "    'holiday': 'argentina_holidays',\n",
    "    'ds': list(argentina_holidays.keys()),\n",
    "    'lower_window': 0,\n",
    "    'upper_window': 0\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el Modelo Prophet\n",
    "Configura el modelo de Prophet con los días festivos y entrenalo con los datos históricos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "# Crear modelo Prophet e incluir los feriados\n",
    "model = Prophet(\n",
    "    holidays=holidays_df,\n",
    "    yearly_seasonality=True,  # Capturar estacionalidad anual\n",
    "    weekly_seasonality=True   # Capturar patrones semanales\n",
    ")\n",
    "\n",
    "# Ajustar el modelo con los datos históricos\n",
    "model.fit(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar Predicciones\n",
    "Genera un DataFrame con las fechas futuras para las que necesitas predicciones. Luego, utiliza el modelo para predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear fechas futuras para predicción (365 días hacia adelante)\n",
    "future = model.make_future_dataframe(periods=365)\n",
    "\n",
    "# Generar predicciones\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Visualizar las predicciones\n",
    "model.plot(forecast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar el Impacto de los Días Festivos\n",
    "Prophet incluye automáticamente los efectos de los días festivos en las predicciones. Puedes visualizar su impacto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_components(forecast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto generará gráficos separados que muestran:\n",
    "\n",
    "La tendencia general.\n",
    "La estacionalidad semanal y anual.\n",
    "El efecto de los días festivos.\n",
    "\n",
    "### Incorporar al Modelo General\n",
    "Si estás combinando Prophet con otros modelos (como Random Forest o XGBoost), puedes exportar las predicciones de Prophet como una nueva característica y usarlas como entrada para el modelo más complejo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar predicciones de Prophet con otras variables\n",
    "data['prophet_pred'] = forecast['yhat']\n",
    "\n",
    "# Continuar con el modelo de Machine Learning\n",
    "X = data[['prophet_pred', 'otra_variable']]\n",
    "y = data['y']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Completo con Prohet y Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from prophet import Prophet\n",
    "import holidays\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuración de conexión a PostgreSQL\n",
    "db_config = {\n",
    "    \"host\": \"tu_host\",\n",
    "    \"port\": \"5432\",  # Puerto por defecto\n",
    "    \"dbname\": \"nombre_base_datos\",\n",
    "    \"user\": \"tu_usuario\",\n",
    "    \"password\": \"tu_contraseña\"\n",
    "}\n",
    "engine = create_engine(\n",
    "    f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\"\n",
    ")\n",
    "\n",
    "# Consulta para cargar datos\n",
    "query = \"\"\"\n",
    "SELECT fecha AS ds, \n",
    "       ventas AS y, \n",
    "       stock_suficiente, \n",
    "       oferta, \n",
    "       precio_relativo, \n",
    "       tienda, \n",
    "       producto\n",
    "FROM ventas_historicas\n",
    "WHERE stock_suficiente = true;\n",
    "\"\"\"\n",
    "\n",
    "# Cargar datos desde PostgreSQL\n",
    "data = pd.read_sql(query, engine)\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n",
    "\n",
    "# Obtener los días festivos de Argentina\n",
    "argentina_holidays = holidays.AR(years=[2023, 2024])\n",
    "holidays_df = pd.DataFrame({\n",
    "    'holiday': 'argentina_holidays',\n",
    "    'ds': list(argentina_holidays.keys()),\n",
    "    'lower_window': 0,\n",
    "    'upper_window': 0\n",
    "})\n",
    "\n",
    "# Crear y entrenar el modelo Prophet\n",
    "prophet_model = Prophet(\n",
    "    holidays=holidays_df,\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True\n",
    ")\n",
    "prophet_model.fit(data)\n",
    "\n",
    "# Crear fechas futuras y generar predicciones con Prophet\n",
    "future = prophet_model.make_future_dataframe(periods=90)  # 90 días hacia adelante\n",
    "forecast = prophet_model.predict(future)\n",
    "\n",
    "# Merge de predicciones de Prophet con los datos originales\n",
    "data = data.merge(forecast[['ds', 'yhat']], on='ds', how='left')\n",
    "data.rename(columns={'yhat': 'pred_demanda_prophet'}, inplace=True)\n",
    "\n",
    "# Agregar variables adicionales (estacionalidad, oferta, precio relativo)\n",
    "data['day_of_week'] = data['ds'].dt.dayofweek\n",
    "data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Crear variables dummy para días de la semana\n",
    "data = pd.get_dummies(data, columns=['day_of_week'], drop_first=True)\n",
    "\n",
    "# Definir las variables independientes (X) y dependientes (y)\n",
    "X = data[['pred_demanda_prophet', 'oferta', 'precio_relativo', 'is_weekend'] + [col for col in data.columns if 'day_of_week_' in col]]\n",
    "y = data['y']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar modelo de Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones y evaluación del modelo\n",
    "y_pred = rf_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE del modelo combinado: {rmse}\")\n",
    "\n",
    "# Importancia de las variables\n",
    "importances = rf_model.feature_importances_\n",
    "plt.barh(X.columns, importances)\n",
    "plt.xlabel(\"Importancia\")\n",
    "plt.ylabel(\"Variables\")\n",
    "plt.title(\"Importancia de las Variables en el Modelo Combinado\")\n",
    "plt.show()\n",
    "\n",
    "# Predicción de la demanda futura usando el modelo completo\n",
    "future_forecast = forecast[['ds', 'yhat']].tail(90)\n",
    "future_forecast['pred_ml'] = rf_model.predict(future_forecast[['yhat']])\n",
    "print(future_forecast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detalles del Modelo\n",
    "1. Conexión a PostgreSQL\n",
    "\n",
    "Los datos se cargan directamente desde la base de datos PostgreSQL, con una consulta SQL que filtra los días sin suficiente stock.\n",
    "\n",
    "2. Prophet\n",
    "\n",
    "Modela la estacionalidad anual, semanal y los efectos de los días festivos en Argentina.\n",
    "Genera predicciones básicas de demanda (columna yhat).\n",
    "\n",
    "3. Machine Learning (Random Forest)\n",
    "\n",
    "Toma las predicciones de Prophet (yhat) como una característica.\n",
    "Incorpora otras variables (ofertas, precios relativos, fines de semana, etc.) para ajustar la predicción a factores adicionales.\n",
    "\n",
    "4. Evaluación del Modelo\n",
    "\n",
    "Usa RMSE para medir el desempeño del modelo combinado.\n",
    "Visualiza la importancia de las variables.\n",
    "\n",
    "5. Predicciones Futuras\n",
    "\n",
    "Genera predicciones futuras basadas en las fechas proyectadas y ajustadas con el modelo de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas del Enfoque\n",
    "**Escalabilidad:** Maneja grandes volúmenes de datos directamente desde la base de datos.\n",
    "\n",
    "**Precisión:** Combina un modelo estadístico (Prophet) con Machine Learning para capturar factores complejos.\n",
    "\n",
    "**Flexibilidad:** Puede extenderse con más variables o diferentes algoritmos de ML.\n",
    "\n",
    "Este modelo es modular y se puede ajustar para satisfacer necesidades específicas, como cambios en la estacionalidad o incorporar nuevas características relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIARCOEST - ESTADISTICA VENTA PLU x PRECIO x LOCAL x DIA \n",
    "\n",
    "SELECT TOP (1000) [F_VENTA]\n",
    "      ,[C_ARTICULO]\n",
    "      ,[C_FAMILIA]\n",
    "      ,[C_SUCU_EMPR]\n",
    "      ,[I_PRECIO_VENTA]\n",
    "      ,[I_PRECIO_COSTO]\n",
    "      ,[I_VENDIDO]\n",
    "      ,[Q_UNIDADES_VENDIDAS]\n",
    "      ,[I_PRECIO_COSTO_PP]\n",
    "      ,[I_PARTE_ULTIMO_INGRESO]\n",
    "      ,[I_COMPRA_ULTIMO_INGRESO]\n",
    "      ,[I_IMP_INTERNOS]\n",
    "  FROM [DiarcoEst].[dbo].[T702_EST_VTAS_POR_ARTICULO]\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Para Funcionas asegurarse de conectarse a la VPN\n",
    "\n",
    "Pass: Z33treex.2024\t\n",
    "\n",
    "DIARCOEST -->  SQLSERVER:192.168.0.250   eettlin/connexa.2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRIVER=ODBC Driver 17 for SQL Server;SERVER =192.168.0.250;BASE=DiarcoEst;USER=eettlin;PWD=connexa.2024\n"
     ]
    }
   ],
   "source": [
    "# OBTENER PARÁMETROS de la CONEXIÓN\n",
    "\n",
    "import pyodbc\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "secrets = dotenv_values(\".env\")\n",
    "local_secrets = dotenv_values(\".env.dev\")\n",
    "\n",
    "DRIVER = secrets[\"DRIVER\"]\n",
    "SERVIDOR = secrets[\"SERVIDOR\"]\n",
    "PUERTO = secrets[\"PUERTO\"]\n",
    "BASE = secrets[\"BASE\"]\n",
    "USUARIO = secrets[\"USUARIO\"]\n",
    "CONTRASENA = secrets[\"CONTRASENA\"]\n",
    "\n",
    "\n",
    "constr = f'DRIVER={DRIVER};SERVER ={SERVIDOR};BASE={BASE};USER={USUARIO};PWD={CONTRASENA}'\n",
    "\n",
    "print (constr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para estimar la demanda de productos en una cadena de supermercados considerando los factores mencionados (stock, ofertas y ventas históricas), se puede desarrollar un modelo utilizando Python y herramientas de análisis de datos y aprendizaje automático. A continuación, se describe cómo abordar el problema, junto con un modelo conceptual y el uso de las herramientas necesarias:\n",
    "## 1. Enfoque General\n",
    "    a. Objetivo\n",
    "    Estimar la demanda diaria esperada de cada producto en cada tienda, ajustada por:\n",
    "        •\tDisponibilidad de stock.\n",
    "        •\tImpacto de las ofertas.\n",
    "        •\tTendencias de ventas históricas.\n",
    "    b. Consideraciones\n",
    "        •\tUsar únicamente datos de días con stock suficiente para calcular el promedio y modelar la demanda base.\n",
    "        •\tCapturar el impacto de las ofertas como un multiplicador en la demanda base.\n",
    "        •\tIncorporar estacionalidad y patrones semanales.\n",
    "    c. Herramientas\n",
    "        •\tPandas: Para manipulación y limpieza de datos.\n",
    "        •\tScikit-learn: Para modelos de regresión lineal y Machine Learning.\n",
    "        •\tStatsmodels: Para modelado de series temporales (opcional).\n",
    "        •\tNumPy y Matplotlib/Seaborn: Para cálculos y visualización.\n",
    "\n",
    "## 2. Diseño del Modelo\n",
    "### a. Variables de Entrada\n",
    "1.\tFecha: Día específico.\n",
    "2.\tTienda: Identificador de la tienda.\n",
    "3.\tProducto: Identificador del producto.\n",
    "4.\tVentas Diarias: Cantidad vendida en ese día.\n",
    "5.\tStock Suficiente (Booleano): Indicador de si había suficiente stock.\n",
    "6.\tEn Oferta (Booleano): Indicador de si el producto estaba en oferta.\n",
    "7.\tPrecio Relativo: Precio del producto en comparación con su precio regular.\n",
    "    El precio relativo es un dato numérico continuo y se utiliza en modelos de predicción como una variable independiente. Es importante normalizar o estandarizar esta variable si se emplea en modelos de Machine Learning que son sensibles a la escala.\n",
    "\n",
    "    Precio Actual / Precio Regular\n",
    "\n",
    "        **Formato de Dato** \n",
    "        Tipo de dato en Python: float\n",
    "        Valores típicos:\n",
    "        < 1.0: Indica descuentos.\n",
    "        1.0: Precio igual al precio regular.\n",
    "        > 1.0: Precio más alto que el regular.\n",
    "8.\tCaracterísticas Externas: Festivos, eventos locales, etc.\n",
    "\n",
    "### b. Preparación de Datos\n",
    "\n",
    "1.\tFiltrar días sin suficiente stock para estimar la demanda base.\n",
    "\n",
    "2.\tCalcular el promedio de ventas diarias por producto-tienda para los días con stock suficiente.\n",
    "\n",
    "3.\tCodificar información sobre ofertas y ajustar la demanda observada.\n",
    "\n",
    "## 3. Modelo Basado en Regresión\n",
    "1.\tRegresión Lineal Múltiple Usar variables como precio relativo, indicador de oferta, día de la semana, y festividad para estimar la demanda diaria.\n",
    "Fórmula básica:\n",
    " \n",
    "Demandat=β0+β1(Ofertat)+β2(Precio Relativot)+β3(Dıˊa de la Semanat)+ϵ\\text{Demanda}_t = \\beta_0 + \\beta_1 (\\text{Oferta}_t) + \\beta_2 (\\text{Precio Relativo}_t) + \\beta_3 (\\text{Día de la Semana}_t) + \\epsilonDemandat=β0+β1(Ofertat)+β2(Precio Relativot)+β3(Dıˊa de la Semanat)+ϵ\n",
    "\n",
    "2.\tAlgoritmos de ML Adicionales\n",
    "o\tRandom Forest: Para modelar relaciones no lineales y manejar interacciones complejas entre variables.\n",
    "o\tXGBoost: Para mejorar la precisión en estimaciones más complejas.\n",
    "3.\tValidación del Modelo Dividir los datos en conjuntos de entrenamiento y prueba. Utilizar métricas como el RMSE o MAE para evaluar la precisión.\n",
    "\n",
    "4. Implementación en Python\n",
    "A continuación, un esquema básico del código:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eduar\\AppData\\Local\\Temp\\ipykernel_26776\\3903340535.py:44: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# Configuración de conexión a SQL Server\n",
    "\n",
    "constr = f'DRIVER={DRIVER};SERVER ={SERVIDOR};PORT={PUERTO};BASE={BASE};USER={USUARIO};PWD={CONTRASENA}'\n",
    "\n",
    "conn_str = (\n",
    "    \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    \"SERVER=192.168.0.250;\"\n",
    "     \"PORT=1433;\"\n",
    "    \"DATABASE=DiarcoEst;\"\n",
    "    \"UID=eettlin;\"\n",
    "    \"PWD=connexa.2024;\"\n",
    ")\n",
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "# Consultar los datos\n",
    "query = \"\"\"\n",
    "SELECT V.[F_VENTA]\n",
    "      ,V.[C_ARTICULO]\n",
    "      ,V.[C_SUCU_EMPR]\n",
    "      ,V.[I_PRECIO_VENTA]\n",
    "      ,V.[I_PRECIO_COSTO]\n",
    "      ,V.[I_VENDIDO]\n",
    "      ,V.[Q_UNIDADES_VENDIDAS]\n",
    "      ,V.[I_PRECIO_COSTO_PP]\n",
    "  \n",
    "      ,V.[C_FAMILIA]\n",
    "      ,A.[C_RUBRO]\n",
    "      ,A.[C_SUBRUBRO_1]\n",
    "      ,A.[C_SUBRUBRO_2]\n",
    "      ,A.[N_ARTICULO]\n",
    " \n",
    "  FROM [DiarcoEst].[dbo].[T702_EST_VTAS_POR_ARTICULO] V\n",
    "  LEFT JOIN [DiarcoEst].[dbo].[T050_ARTICULOS] A \n",
    "\tON V.C_ARTICULO = A.C_ARTICULO\n",
    "WHERE V.C_ARTICULO BETWEEN  50 AND 100  AND\n",
    "V.F_VENTA >='20240101'\n",
    "\"\"\"\n",
    "data = pd.read_sql(query, conn)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45006 entries, 0 to 45005\n",
      "Data columns (total 16 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   F_VENTA                  45006 non-null  datetime64[ns]\n",
      " 1   C_ARTICULO               45006 non-null  float64       \n",
      " 2   C_FAMILIA                45006 non-null  float64       \n",
      " 3   C_SUCU_EMPR              45006 non-null  float64       \n",
      " 4   I_PRECIO_VENTA           45006 non-null  float64       \n",
      " 5   I_PRECIO_COSTO           45006 non-null  float64       \n",
      " 6   I_VENDIDO                45006 non-null  float64       \n",
      " 7   Q_UNIDADES_VENDIDAS      45006 non-null  float64       \n",
      " 8   I_PRECIO_COSTO_PP        45006 non-null  float64       \n",
      " 9   I_PARTE_ULTIMO_INGRESO   45006 non-null  float64       \n",
      " 10  I_COMPRA_ULTIMO_INGRESO  45006 non-null  float64       \n",
      " 11  I_IMP_INTERNOS           45006 non-null  float64       \n",
      " 12  C_RUBRO                  45006 non-null  float64       \n",
      " 13  C_SUBRUBRO_1             45006 non-null  float64       \n",
      " 14  C_SUBRUBRO_2             45006 non-null  float64       \n",
      " 15  N_ARTICULO               45006 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(14), object(1)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# data.describe()\n",
    "\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Procesamiento de los Datos\n",
    "\n",
    "Cálculo del Precio Relativo\n",
    "\n",
    "Usando I_PRECIO_COSTO_PP como referencia del precio regular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['precio_relativo'] = data['I_PRECIO_VENTA'] / data['I_PRECIO_COSTO_PP']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables Temporales\n",
    "Extraer información adicional de la fecha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['F_VENTA'] = pd.to_datetime(data['F_VENTA'])\n",
    "data['dia_semana'] = data['F_VENTA'].dt.dayofweek  # 0=Lunes, 6=Domingo\n",
    "data['es_fin_semana'] = data['dia_semana'].isin([5, 6]).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detección de Promociones\n",
    "Detectar días con precios significativamente más bajos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "precio_base = data.groupby('C_ARTICULO')['I_PRECIO_VENTA'].transform('mean')\n",
    "data['en_promocion'] = (data['I_PRECIO_VENTA'] < 0.9 * precio_base).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporación de Festivos\n",
    "Agregar días festivos con el paquete holidays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eduar\\AppData\\Local\\Temp\\ipykernel_26776\\308624387.py:5: FutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n",
      "  data['es_feriado'] = data['F_VENTA'].isin(argentina_holidays).astype(int)\n"
     ]
    }
   ],
   "source": [
    "import holidays\n",
    "\n",
    "# Obtener feriados de Argentina\n",
    "argentina_holidays = holidays.AR(years=[2023, 2024])\n",
    "data['es_feriado'] = data['F_VENTA'].isin(argentina_holidays).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construcción del Modelo\n",
    "1. Variables de Entrada (X)\n",
    "\n",
    "precio_relativo, en_promocion, es_feriado, es_fin_semana, dia_semana, I_IMP_INTERNOS.\n",
    "\n",
    "2. Variable de Salida (y)\n",
    "\n",
    "Q_UNIDADES_VENDIDAS.\n",
    "Entrenamiento y Predicción Usa un modelo de regresión o aprendizaje automático para predecir la demanda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Variables independientes y dependientes\n",
    "X = data[['precio_relativo', 'en_promocion', 'es_feriado', 'es_fin_semana', 'dia_semana', 'I_IMP_INTERNOS']]\n",
    "y = data['Q_UNIDADES_VENDIDAS']\n",
    "\n",
    "# División de los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo de Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Escalabilidad\n",
    "Este enfoque puede escalar fácilmente al usar:\n",
    "\n",
    "Procesamiento en SQL Server:\n",
    "Realizar cálculos de agregación y filtrado directamente en la base de datos antes de cargar los datos.\n",
    "Procesamiento en Chunks:\n",
    "Cargar datos en porciones si son muy grandes para manejar en memoria.\n",
    "Este pipeline combina consultas SQL, procesamiento de datos en pandas y aprendizaje automático para construir un modelo efectivo de predicción de demanda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cálculo de Elasticidad con los Datos Disponibles\n",
    "Dado que la tabla contiene diferentes precios para el mismo PLU en un mismo día, puedes calcular el cambio relativo en el precio y en la cantidad vendida.\n",
    "\n",
    "Paso 1: Agrupación de Datos\n",
    "Agrupa los datos por PLU y fecha para comparar las ventas y precios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar datos (asumiendo conexión o CSV)\n",
    "data = pd.DataFrame({\n",
    "    'F_VENTA': ['2024-01-01', '2024-01-01', '2024-01-01', '2024-01-02'],\n",
    "    'C_ARTICULO': [101, 101, 102, 101],\n",
    "    'I_PRECIO_VENTA': [100, 120, 200, 100],\n",
    "    'Q_UNIDADES_VENDIDAS': [50, 40, 30, 60]\n",
    "})\n",
    "\n",
    "# Agrupar por artículo y fecha\n",
    "grouped = data.groupby(['F_VENTA', 'C_ARTICULO'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2: Calcular Diferencias de Precio y Cantidad\n",
    "Para cada grupo, calcula los cambios relativos en precio y cantidad:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar por fecha y PLU\n",
    "data.sort_values(by=['F_VENTA', 'C_ARTICULO', 'I_PRECIO_VENTA'], inplace=True)\n",
    "\n",
    "# Calcular diferencias en precio y cantidad\n",
    "data['delta_precio'] = data.groupby(['F_VENTA', 'C_ARTICULO'])['I_PRECIO_VENTA'].diff()\n",
    "data['delta_cantidad'] = data.groupby(['F_VENTA', 'C_ARTICULO'])['Q_UNIDADES_VENDIDAS'].diff()\n",
    "\n",
    "# Calcular promedios de precio y cantidad (base para porcentaje)\n",
    "data['precio_promedio'] = data.groupby(['F_VENTA', 'C_ARTICULO'])['I_PRECIO_VENTA'].transform('mean')\n",
    "data['cantidad_promedio'] = data.groupby(['F_VENTA', 'C_ARTICULO'])['Q_UNIDADES_VENDIDAS'].transform('mean')\n",
    "\n",
    "# Calcular elasticidad\n",
    "data['elasticidad'] = (\n",
    "    (data['delta_cantidad'] / data['cantidad_promedio']) /\n",
    "    (data['delta_precio'] / data['precio_promedio'])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualización y Segmentación\n",
    "Elasticidad por Producto\n",
    "Agrupa los valores de elasticidad para obtener una visión general por producto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_ARTICULO\n",
      "50.0     1.266666\n",
      "52.0          NaN\n",
      "62.0          NaN\n",
      "74.0          NaN\n",
      "75.0          NaN\n",
      "80.0          NaN\n",
      "82.0          NaN\n",
      "86.0          inf\n",
      "88.0          NaN\n",
      "93.0          NaN\n",
      "100.0         NaN\n",
      "Name: elasticidad, dtype: float64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "supplied range of [-inf, inf] is not finite",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Histograma de elasticidades\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43melasticidad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistribución de Elasticidades de Precio\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElasticidad\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32me:\\PY\\.venv\\Lib\\site-packages\\pandas\\plotting\\_core.py:129\u001b[0m, in \u001b[0;36mhist_series\u001b[1;34m(self, by, ax, grid, xlabelsize, xrot, ylabelsize, yrot, figsize, bins, backend, legend, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03mDraw histogram of the input series using matplotlib.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m    >>> hist = ser.groupby(level=0).hist()\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m plot_backend \u001b[38;5;241m=\u001b[39m _get_plot_backend(backend)\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mplot_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhist_series\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxlabelsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxlabelsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxrot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxrot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mylabelsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mylabelsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43myrot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myrot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfigsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\PY\\.venv\\Lib\\site-packages\\pandas\\plotting\\_matplotlib\\hist.py:454\u001b[0m, in \u001b[0;36mhist_series\u001b[1;34m(self, by, ax, grid, xlabelsize, xrot, ylabelsize, yrot, figsize, bins, legend, **kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m legend:\n\u001b[0;32m    453\u001b[0m     kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m--> 454\u001b[0m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m legend:\n\u001b[0;32m    456\u001b[0m     ax\u001b[38;5;241m.\u001b[39mlegend()\n",
      "File \u001b[1;32me:\\PY\\.venv\\Lib\\site-packages\\matplotlib\\__init__.py:1476\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1476\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1477\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1478\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1479\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1481\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1482\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1483\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32me:\\PY\\.venv\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:7001\u001b[0m, in \u001b[0;36mAxes.hist\u001b[1;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[0;32m   6997\u001b[0m \u001b[38;5;66;03m# Loop through datasets\u001b[39;00m\n\u001b[0;32m   6998\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nx):\n\u001b[0;32m   6999\u001b[0m     \u001b[38;5;66;03m# this will automatically overwrite bins,\u001b[39;00m\n\u001b[0;32m   7000\u001b[0m     \u001b[38;5;66;03m# so that each histogram uses the same bins\u001b[39;00m\n\u001b[1;32m-> 7001\u001b[0m     m, bins \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhist_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   7002\u001b[0m     tops\u001b[38;5;241m.\u001b[39mappend(m)\n\u001b[0;32m   7003\u001b[0m tops \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(tops, \u001b[38;5;28mfloat\u001b[39m)  \u001b[38;5;66;03m# causes problems later if it's an int\u001b[39;00m\n",
      "File \u001b[1;32me:\\PY\\.venv\\Lib\\site-packages\\numpy\\lib\\histograms.py:780\u001b[0m, in \u001b[0;36mhistogram\u001b[1;34m(a, bins, range, density, weights)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;124;03mCompute the histogram of a dataset.\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    776\u001b[0m \n\u001b[0;32m    777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    778\u001b[0m a, weights \u001b[38;5;241m=\u001b[39m _ravel_and_check_weights(a, weights)\n\u001b[1;32m--> 780\u001b[0m bin_edges, uniform_bins \u001b[38;5;241m=\u001b[39m \u001b[43m_get_bin_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;66;03m# Histogram is an integer or a float array depending on the weights.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\PY\\.venv\\Lib\\site-packages\\numpy\\lib\\histograms.py:426\u001b[0m, in \u001b[0;36m_get_bin_edges\u001b[1;34m(a, bins, range, weights)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_equal_bins \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins` must be positive, when an integer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 426\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m \u001b[43m_get_outer_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(bins) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    429\u001b[0m     bin_edges \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(bins)\n",
      "File \u001b[1;32me:\\PY\\.venv\\Lib\\site-packages\\numpy\\lib\\histograms.py:315\u001b[0m, in \u001b[0;36m_get_outer_edges\u001b[1;34m(a, range)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax must be larger than min in range parameter.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (np\u001b[38;5;241m.\u001b[39misfinite(first_edge) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(last_edge)):\n\u001b[1;32m--> 315\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    316\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupplied range of [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] is not finite\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(first_edge, last_edge))\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m a\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# handle empty arrays. Can't determine range, so use 0-1.\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: supplied range of [-inf, inf] is not finite"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "elasticity_summary = data.groupby('C_ARTICULO')['elasticidad'].mean()\n",
    "print(elasticity_summary)\n",
    "\n",
    "### VISUALIZACIÓN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Histograma de elasticidades\n",
    "data['elasticidad'].dropna().hist(bins=20)\n",
    "plt.title('Distribución de Elasticidades de Precio')\n",
    "plt.xlabel('Elasticidad')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "## Elasticidad por artículo\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
