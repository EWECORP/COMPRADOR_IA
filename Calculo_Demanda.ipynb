{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para estimar la demanda de productos en una cadena de supermercados considerando los factores mencionados (stock, ofertas y ventas históricas), se puede desarrollar un modelo utilizando Python y herramientas de análisis de datos y aprendizaje automático. A continuación, se describe cómo abordar el problema, junto con un modelo conceptual y el uso de las herramientas necesarias:\n",
    "## 1. Enfoque General\n",
    "    a. Objetivo\n",
    "    Estimar la demanda diaria esperada de cada producto en cada tienda, ajustada por:\n",
    "        •\tDisponibilidad de stock.\n",
    "        •\tImpacto de las ofertas.\n",
    "        •\tTendencias de ventas históricas.\n",
    "    b. Consideraciones\n",
    "        •\tUsar únicamente datos de días con stock suficiente para calcular el promedio y modelar la demanda base.\n",
    "        •\tCapturar el impacto de las ofertas como un multiplicador en la demanda base.\n",
    "        •\tIncorporar estacionalidad y patrones semanales.\n",
    "    c. Herramientas\n",
    "        •\tPandas: Para manipulación y limpieza de datos.\n",
    "        •\tScikit-learn: Para modelos de regresión lineal y Machine Learning.\n",
    "        •\tStatsmodels: Para modelado de series temporales (opcional).\n",
    "        •\tNumPy y Matplotlib/Seaborn: Para cálculos y visualización.\n",
    "\n",
    "## 2. Diseño del Modelo\n",
    "### a. Variables de Entrada\n",
    "1.\tFecha: Día específico.\n",
    "2.\tTienda: Identificador de la tienda.\n",
    "3.\tProducto: Identificador del producto.\n",
    "4.\tVentas Diarias: Cantidad vendida en ese día.\n",
    "5.\tStock Suficiente (Booleano): Indicador de si había suficiente stock.\n",
    "6.\tEn Oferta (Booleano): Indicador de si el producto estaba en oferta.\n",
    "7.\tPrecio Relativo: Precio del producto en comparación con su precio regular.\n",
    "    El precio relativo es un dato numérico continuo y se utiliza en modelos de predicción como una variable independiente. Es importante normalizar o estandarizar esta variable si se emplea en modelos de Machine Learning que son sensibles a la escala.\n",
    "\n",
    "    Precio Actual / Precio Regular\n",
    "\n",
    "        **Formato de Dato** \n",
    "        Tipo de dato en Python: float\n",
    "        Valores típicos:\n",
    "        < 1.0: Indica descuentos.\n",
    "        1.0: Precio igual al precio regular.\n",
    "        > 1.0: Precio más alto que el regular.\n",
    "8.\tCaracterísticas Externas: Festivos, eventos locales, etc.\n",
    "\n",
    "### b. Preparación de Datos\n",
    "\n",
    "1.\tFiltrar días sin suficiente stock para estimar la demanda base.\n",
    "\n",
    "2.\tCalcular el promedio de ventas diarias por producto-tienda para los días con stock suficiente.\n",
    "\n",
    "3.\tCodificar información sobre ofertas y ajustar la demanda observada.\n",
    "\n",
    "## 3. Modelo Basado en Regresión\n",
    "1.\tRegresión Lineal Múltiple Usar variables como precio relativo, indicador de oferta, día de la semana, y festividad para estimar la demanda diaria.\n",
    "Fórmula básica:\n",
    " \n",
    "Demandat=β0+β1(Ofertat)+β2(Precio Relativot)+β3(Dıˊa de la Semanat)+ϵ\\text{Demanda}_t = \\beta_0 + \\beta_1 (\\text{Oferta}_t) + \\beta_2 (\\text{Precio Relativo}_t) + \\beta_3 (\\text{Día de la Semana}_t) + \\epsilonDemandat=β0+β1(Ofertat)+β2(Precio Relativot)+β3(Dıˊa de la Semanat)+ϵ\n",
    "\n",
    "2.\tAlgoritmos de ML Adicionales\n",
    "o\tRandom Forest: Para modelar relaciones no lineales y manejar interacciones complejas entre variables.\n",
    "o\tXGBoost: Para mejorar la precisión en estimaciones más complejas.\n",
    "3.\tValidación del Modelo Dividir los datos en conjuntos de entrenamiento y prueba. Utilizar métricas como el RMSE o MAE para evaluar la precisión.\n",
    "\n",
    "4. Implementación en Python\n",
    "A continuación, un esquema básico del código:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv(\"ventas_historicas.csv\")\n",
    "\n",
    "# Filtrar días con stock suficiente\n",
    "data = data[data['stock_suficiente'] == 1]\n",
    "\n",
    "# Crear variables de tiempo\n",
    "data['day_of_week'] = pd.to_datetime(data['fecha']).dt.dayofweek\n",
    "data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Variables dummies para ofertas y estacionalidad\n",
    "data['oferta'] = data['oferta'].astype(int)\n",
    "data = pd.get_dummies(data, columns=['day_of_week'], drop_first=True)\n",
    "\n",
    "# Definir variables independientes y dependientes\n",
    "X = data[['oferta', 'precio_relativo', 'is_weekend'] + [col for col in data.columns if 'day_of_week_' in col]]\n",
    "y = data['ventas']\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar un modelo de Random Forest\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones y evaluación\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"RMSE del modelo: {rmse}\")\n",
    "\n",
    "# Importancia de las variables\n",
    "importances = model.feature_importances_\n",
    "plt.barh(X.columns, importances)\n",
    "plt.xlabel(\"Importancia\")\n",
    "plt.ylabel(\"Variables\")\n",
    "plt.title(\"Importancia de las Variables en el Modelo\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizar POSTGRE para Acelerar Cálculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Configurar la conexión\n",
    "db_config = {\n",
    "    \"host\": \"tu_host\",\n",
    "    \"port\": \"5432\",  # Puerto por defecto\n",
    "    \"dbname\": \"nombre_base_datos\",\n",
    "    \"user\": \"tu_usuario\",\n",
    "    \"password\": \"tu_contraseña\"\n",
    "}\n",
    "\n",
    "# Crear la conexión usando SQLAlchemy\n",
    "engine = create_engine(\n",
    "    f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\"\n",
    ")\n",
    "\n",
    "# Probar la conexión y cargar datos directamente\n",
    "query = \"\"\"\n",
    "SELECT fecha, tienda, producto, ventas, stock_suficiente, oferta, precio_relativo\n",
    "FROM ventas_historicas\n",
    "WHERE stock_suficiente = true;  -- Filtrar días con stock suficiente\n",
    "\"\"\"\n",
    "\n",
    "# Cargar datos en un DataFrame de pandas\n",
    "data = pd.read_sql(query, engine)\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesar Datos en la Base de Datos \n",
    "Aprovecha la potencia de PostgreSQL para realizar cálculos y filtrados directamente en la base de datos antes de cargar los datos en Python. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql \n",
    "SELECT tienda, producto, AVG(ventas) AS promedio_ventas\n",
    "FROM ventas_historicas\n",
    "WHERE stock_suficiente = true\n",
    "GROUP BY tienda, producto;\n",
    "\n",
    "#sql\n",
    "SELECT tienda, producto, AVG(ventas) AS promedio_ventas\n",
    "FROM ventas_historicas\n",
    "WHERE stock_suficiente = true\n",
    "GROUP BY tienda, producto;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga Parcial de Datos \n",
    "Si los datos son demasiado grandes para cargarlos todos a la vez, puedes dividirlos en \"chunks\" (porciones) utilizando la función chunksize de pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in pd.read_sql(query, engine, chunksize=10000):\n",
    "    # Procesar cada porción por separado\n",
    "    procesar_chunk(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporar Estacionalidad con PROPHET\n",
    "\n",
    "Prophet es especialmente útil para capturar estacionalidades, tendencias y efectos de días festivos. En tu caso, puedes añadir los días festivos de Argentina como una de las entradas del modelo.\n",
    "\n",
    "### Preparación de los Datos\n",
    "Asegúrate de tener los datos en un formato adecuado para Prophet. Necesitas al menos dos columnas:\n",
    "\n",
    "* ds: Fecha (en formato YYYY-MM-DD).\n",
    "* y: Variable objetivo (en este caso, la demanda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar datos desde PostgreSQL o un archivo\n",
    "data = pd.DataFrame({\n",
    "    'ds': ['2024-01-01', '2024-01-02', '2024-01-03'],  # Fechas\n",
    "    'y': [100, 120, 110]  # Ventas\n",
    "})\n",
    "\n",
    "# Convertir 'ds' al formato de fecha\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporar los Días Festivos de Argentina\n",
    "Prophet permite añadir efectos de días festivos a nivel nacional. Puedes usar un paquete como holidays para obtener los feriados locales de Argentina:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "# Obtener los días festivos de Argentina\n",
    "argentina_holidays = holidays.AR(years=[2023, 2024])  # Ajusta los años necesarios\n",
    "\n",
    "# Crear un DataFrame con los días festivos\n",
    "holidays_df = pd.DataFrame({\n",
    "    'holiday': 'argentina_holidays',\n",
    "    'ds': list(argentina_holidays.keys()),\n",
    "    'lower_window': 0,\n",
    "    'upper_window': 0\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el Modelo Prophet\n",
    "Configura el modelo de Prophet con los días festivos y entrenalo con los datos históricos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "# Crear modelo Prophet e incluir los feriados\n",
    "model = Prophet(\n",
    "    holidays=holidays_df,\n",
    "    yearly_seasonality=True,  # Capturar estacionalidad anual\n",
    "    weekly_seasonality=True   # Capturar patrones semanales\n",
    ")\n",
    "\n",
    "# Ajustar el modelo con los datos históricos\n",
    "model.fit(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar Predicciones\n",
    "Genera un DataFrame con las fechas futuras para las que necesitas predicciones. Luego, utiliza el modelo para predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear fechas futuras para predicción (365 días hacia adelante)\n",
    "future = model.make_future_dataframe(periods=365)\n",
    "\n",
    "# Generar predicciones\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Visualizar las predicciones\n",
    "model.plot(forecast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar el Impacto de los Días Festivos\n",
    "Prophet incluye automáticamente los efectos de los días festivos en las predicciones. Puedes visualizar su impacto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_components(forecast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto generará gráficos separados que muestran:\n",
    "\n",
    "La tendencia general.\n",
    "La estacionalidad semanal y anual.\n",
    "El efecto de los días festivos.\n",
    "\n",
    "### Incorporar al Modelo General\n",
    "Si estás combinando Prophet con otros modelos (como Random Forest o XGBoost), puedes exportar las predicciones de Prophet como una nueva característica y usarlas como entrada para el modelo más complejo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar predicciones de Prophet con otras variables\n",
    "data['prophet_pred'] = forecast['yhat']\n",
    "\n",
    "# Continuar con el modelo de Machine Learning\n",
    "X = data[['prophet_pred', 'otra_variable']]\n",
    "y = data['y']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Completo con Prohet y Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from prophet import Prophet\n",
    "import holidays\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuración de conexión a PostgreSQL\n",
    "db_config = {\n",
    "    \"host\": \"tu_host\",\n",
    "    \"port\": \"5432\",  # Puerto por defecto\n",
    "    \"dbname\": \"nombre_base_datos\",\n",
    "    \"user\": \"tu_usuario\",\n",
    "    \"password\": \"tu_contraseña\"\n",
    "}\n",
    "engine = create_engine(\n",
    "    f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\"\n",
    ")\n",
    "\n",
    "# Consulta para cargar datos\n",
    "query = \"\"\"\n",
    "SELECT fecha AS ds, \n",
    "       ventas AS y, \n",
    "       stock_suficiente, \n",
    "       oferta, \n",
    "       precio_relativo, \n",
    "       tienda, \n",
    "       producto\n",
    "FROM ventas_historicas\n",
    "WHERE stock_suficiente = true;\n",
    "\"\"\"\n",
    "\n",
    "# Cargar datos desde PostgreSQL\n",
    "data = pd.read_sql(query, engine)\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n",
    "\n",
    "# Obtener los días festivos de Argentina\n",
    "argentina_holidays = holidays.AR(years=[2023, 2024])\n",
    "holidays_df = pd.DataFrame({\n",
    "    'holiday': 'argentina_holidays',\n",
    "    'ds': list(argentina_holidays.keys()),\n",
    "    'lower_window': 0,\n",
    "    'upper_window': 0\n",
    "})\n",
    "\n",
    "# Crear y entrenar el modelo Prophet\n",
    "prophet_model = Prophet(\n",
    "    holidays=holidays_df,\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True\n",
    ")\n",
    "prophet_model.fit(data)\n",
    "\n",
    "# Crear fechas futuras y generar predicciones con Prophet\n",
    "future = prophet_model.make_future_dataframe(periods=90)  # 90 días hacia adelante\n",
    "forecast = prophet_model.predict(future)\n",
    "\n",
    "# Merge de predicciones de Prophet con los datos originales\n",
    "data = data.merge(forecast[['ds', 'yhat']], on='ds', how='left')\n",
    "data.rename(columns={'yhat': 'pred_demanda_prophet'}, inplace=True)\n",
    "\n",
    "# Agregar variables adicionales (estacionalidad, oferta, precio relativo)\n",
    "data['day_of_week'] = data['ds'].dt.dayofweek\n",
    "data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Crear variables dummy para días de la semana\n",
    "data = pd.get_dummies(data, columns=['day_of_week'], drop_first=True)\n",
    "\n",
    "# Definir las variables independientes (X) y dependientes (y)\n",
    "X = data[['pred_demanda_prophet', 'oferta', 'precio_relativo', 'is_weekend'] + [col for col in data.columns if 'day_of_week_' in col]]\n",
    "y = data['y']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar modelo de Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones y evaluación del modelo\n",
    "y_pred = rf_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE del modelo combinado: {rmse}\")\n",
    "\n",
    "# Importancia de las variables\n",
    "importances = rf_model.feature_importances_\n",
    "plt.barh(X.columns, importances)\n",
    "plt.xlabel(\"Importancia\")\n",
    "plt.ylabel(\"Variables\")\n",
    "plt.title(\"Importancia de las Variables en el Modelo Combinado\")\n",
    "plt.show()\n",
    "\n",
    "# Predicción de la demanda futura usando el modelo completo\n",
    "future_forecast = forecast[['ds', 'yhat']].tail(90)\n",
    "future_forecast['pred_ml'] = rf_model.predict(future_forecast[['yhat']])\n",
    "print(future_forecast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detalles del Modelo\n",
    "1. Conexión a PostgreSQL\n",
    "\n",
    "Los datos se cargan directamente desde la base de datos PostgreSQL, con una consulta SQL que filtra los días sin suficiente stock.\n",
    "\n",
    "2. Prophet\n",
    "\n",
    "Modela la estacionalidad anual, semanal y los efectos de los días festivos en Argentina.\n",
    "Genera predicciones básicas de demanda (columna yhat).\n",
    "\n",
    "3. Machine Learning (Random Forest)\n",
    "\n",
    "Toma las predicciones de Prophet (yhat) como una característica.\n",
    "Incorpora otras variables (ofertas, precios relativos, fines de semana, etc.) para ajustar la predicción a factores adicionales.\n",
    "\n",
    "4. Evaluación del Modelo\n",
    "\n",
    "Usa RMSE para medir el desempeño del modelo combinado.\n",
    "Visualiza la importancia de las variables.\n",
    "\n",
    "5. Predicciones Futuras\n",
    "\n",
    "Genera predicciones futuras basadas en las fechas proyectadas y ajustadas con el modelo de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas del Enfoque\n",
    "**Escalabilidad:** Maneja grandes volúmenes de datos directamente desde la base de datos.\n",
    "\n",
    "**Precisión:** Combina un modelo estadístico (Prophet) con Machine Learning para capturar factores complejos.\n",
    "\n",
    "**Flexibilidad:** Puede extenderse con más variables o diferentes algoritmos de ML.\n",
    "\n",
    "Este modelo es modular y se puede ajustar para satisfacer necesidades específicas, como cambios en la estacionalidad o incorporar nuevas características relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIARCOEST - ESTADISTICA VENTA PLU x PRECIO x LOCAL x DIA \n",
    "\n",
    "SELECT TOP (1000) [F_VENTA]\n",
    "      ,[C_ARTICULO]\n",
    "      ,[C_FAMILIA]\n",
    "      ,[C_SUCU_EMPR]\n",
    "      ,[I_PRECIO_VENTA]\n",
    "      ,[I_PRECIO_COSTO]\n",
    "      ,[I_VENDIDO]\n",
    "      ,[Q_UNIDADES_VENDIDAS]\n",
    "      ,[I_PRECIO_COSTO_PP]\n",
    "      ,[I_PARTE_ULTIMO_INGRESO]\n",
    "      ,[I_COMPRA_ULTIMO_INGRESO]\n",
    "      ,[I_IMP_INTERNOS]\n",
    "  FROM [DiarcoEst].[dbo].[T702_EST_VTAS_POR_ARTICULO]\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# Configuración de conexión a SQL Server\n",
    "conn_str = (\n",
    "    \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    \"SERVER=tu_servidor;\"\n",
    "    \"DATABASE=DiarcoEst;\"\n",
    "    \"UID=tu_usuario;\"\n",
    "    \"PWD=tu_contraseña;\"\n",
    ")\n",
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "# Consultar los datos\n",
    "query = \"\"\"\n",
    "SELECT F_VENTA, \n",
    "       C_ARTICULO, \n",
    "       C_FAMILIA, \n",
    "       C_SUCU_EMPR, \n",
    "       I_PRECIO_VENTA, \n",
    "       I_PRECIO_COSTO, \n",
    "       I_VENDIDO, \n",
    "       Q_UNIDADES_VENDIDAS, \n",
    "       I_PRECIO_COSTO_PP, \n",
    "       I_PARTE_ULTIMO_INGRESO, \n",
    "       I_COMPRA_ULTIMO_INGRESO, \n",
    "       I_IMP_INTERNOS\n",
    "FROM [DiarcoEst].[dbo].[T702_EST_VTAS_POR_ARTICULO];\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "SELECT TOP (1000) V.[F_VENTA]\n",
    "      ,V.[C_ARTICULO]\n",
    "---      ,V.[C_FAMILIA]\n",
    "      ,V.[C_SUCU_EMPR]\n",
    "      ,V.[I_PRECIO_VENTA]\n",
    "      ,V.[I_PRECIO_COSTO]\n",
    "      ,V.[I_VENDIDO]\n",
    "      ,V.[Q_UNIDADES_VENDIDAS]\n",
    "      ,V.[I_PRECIO_COSTO_PP]\n",
    "      ,V.[I_PARTE_ULTIMO_INGRESO]\n",
    "      ,V.[I_COMPRA_ULTIMO_INGRESO]\n",
    "      ,V.[I_IMP_INTERNOS]\n",
    "\n",
    "\t  ,A.[C_ARTICULO]\n",
    "      ,A.[C_FAMILIA]\n",
    "      ,A.[C_RUBRO]\n",
    "      ,A.[C_SUBRUBRO_1]\n",
    "      ,A.[C_SUBRUBRO_2]\n",
    "      ,A.[N_ARTICULO]\n",
    " \n",
    "  FROM [DiarcoEst].[dbo].[T702_EST_VTAS_POR_ARTICULO] V\n",
    "  LEFT JOIN [DiarcoEst].[dbo].[T050_ARTICULOS] A \n",
    "\tON V.C_ARTICULO = A.C_ARTICULO\n",
    "WHERE V.C_ARTICULO BETWEEN  50 AND 100  AND\n",
    "V.F_VENTA >='20240101'\n",
    "\"\"\"\n",
    "data = pd.read_sql(query, conn)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Procesamiento de los Datos\n",
    "\n",
    "Cálculo del Precio Relativo\n",
    "\n",
    "Usando I_PRECIO_COSTO_PP como referencia del precio regular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['precio_relativo'] = data['I_PRECIO_VENTA'] / data['I_PRECIO_COSTO_PP']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables Temporales\n",
    "Extraer información adicional de la fecha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['F_VENTA'] = pd.to_datetime(data['F_VENTA'])\n",
    "data['dia_semana'] = data['F_VENTA'].dt.dayofweek  # 0=Lunes, 6=Domingo\n",
    "data['es_fin_semana'] = data['dia_semana'].isin([5, 6]).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detección de Promociones\n",
    "Detectar días con precios significativamente más bajos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precio_base = data.groupby('C_ARTICULO')['I_PRECIO_VENTA'].transform('mean')\n",
    "data['en_promocion'] = (data['I_PRECIO_VENTA'] < 0.9 * precio_base).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporación de Festivos\n",
    "Agregar días festivos con el paquete holidays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "# Obtener feriados de Argentina\n",
    "argentina_holidays = holidays.AR(years=[2023, 2024])\n",
    "data['es_feriado'] = data['F_VENTA'].isin(argentina_holidays).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construcción del Modelo\n",
    "1. Variables de Entrada (X)\n",
    "\n",
    "precio_relativo, en_promocion, es_feriado, es_fin_semana, dia_semana, I_IMP_INTERNOS.\n",
    "\n",
    "2. Variable de Salida (y)\n",
    "\n",
    "Q_UNIDADES_VENDIDAS.\n",
    "Entrenamiento y Predicción Usa un modelo de regresión o aprendizaje automático para predecir la demanda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Variables independientes y dependientes\n",
    "X = data[['precio_relativo', 'en_promocion', 'es_feriado', 'es_fin_semana', 'dia_semana', 'I_IMP_INTERNOS']]\n",
    "y = data['Q_UNIDADES_VENDIDAS']\n",
    "\n",
    "# División de los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo de Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Escalabilidad\n",
    "Este enfoque puede escalar fácilmente al usar:\n",
    "\n",
    "Procesamiento en SQL Server:\n",
    "Realizar cálculos de agregación y filtrado directamente en la base de datos antes de cargar los datos.\n",
    "Procesamiento en Chunks:\n",
    "Cargar datos en porciones si son muy grandes para manejar en memoria.\n",
    "Este pipeline combina consultas SQL, procesamiento de datos en pandas y aprendizaje automático para construir un modelo efectivo de predicción de demanda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cálculo de Elasticidad con los Datos Disponibles\n",
    "Dado que la tabla contiene diferentes precios para el mismo PLU en un mismo día, puedes calcular el cambio relativo en el precio y en la cantidad vendida.\n",
    "\n",
    "Paso 1: Agrupación de Datos\n",
    "Agrupa los datos por PLU y fecha para comparar las ventas y precios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar datos (asumiendo conexión o CSV)\n",
    "data = pd.DataFrame({\n",
    "    'F_VENTA': ['2024-01-01', '2024-01-01', '2024-01-01', '2024-01-02'],\n",
    "    'C_ARTICULO': [101, 101, 102, 101],\n",
    "    'I_PRECIO_VENTA': [100, 120, 200, 100],\n",
    "    'Q_UNIDADES_VENDIDAS': [50, 40, 30, 60]\n",
    "})\n",
    "\n",
    "# Agrupar por artículo y fecha\n",
    "grouped = data.groupby(['F_VENTA', 'C_ARTICULO'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2: Calcular Diferencias de Precio y Cantidad\n",
    "Para cada grupo, calcula los cambios relativos en precio y cantidad:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar por fecha y PLU\n",
    "data.sort_values(by=['F_VENTA', 'C_ARTICULO', 'I_PRECIO_VENTA'], inplace=True)\n",
    "\n",
    "# Calcular diferencias en precio y cantidad\n",
    "data['delta_precio'] = data.groupby(['F_VENTA', 'C_ARTICULO'])['I_PRECIO_VENTA'].diff()\n",
    "data['delta_cantidad'] = data.groupby(['F_VENTA', 'C_ARTICULO'])['Q_UNIDADES_VENDIDAS'].diff()\n",
    "\n",
    "# Calcular promedios de precio y cantidad (base para porcentaje)\n",
    "data['precio_promedio'] = data.groupby(['F_VENTA', 'C_ARTICULO'])['I_PRECIO_VENTA'].transform('mean')\n",
    "data['cantidad_promedio'] = data.groupby(['F_VENTA', 'C_ARTICULO'])['Q_UNIDADES_VENDIDAS'].transform('mean')\n",
    "\n",
    "# Calcular elasticidad\n",
    "data['elasticidad'] = (\n",
    "    (data['delta_cantidad'] / data['cantidad_promedio']) /\n",
    "    (data['delta_precio'] / data['precio_promedio'])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualización y Segmentación\n",
    "Elasticidad por Producto\n",
    "Agrupa los valores de elasticidad para obtener una visión general por producto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticity_summary = data.groupby('C_ARTICULO')['elasticidad'].mean()\n",
    "print(elasticity_summary)\n",
    "\n",
    "### VISUALIZACIÓN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Histograma de elasticidades\n",
    "data['elasticidad'].dropna().hist(bins=20)\n",
    "plt.title('Distribución de Elasticidades de Precio')\n",
    "plt.xlabel('Elasticidad')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "## Elasticidad por artículo\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
